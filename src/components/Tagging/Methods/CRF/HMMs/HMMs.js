//noinspection JSUnresolvedVariable
import React, {Component} from 'react';
import FigRef from './../../../../Figures/FigRef'
import FigImg from './../../../../Figures/Image/Image'
import figs from './../../../../Figures/figs'
import ref from '../../../../Bibliography/References/references'
import bib from  '../../../../Bibliography/bib';
import F from  '../../../../Math/Math';
import chapters from  '../../../../../../chapters';
import abbrs from  '../../../../abbreviations';
import sectionz from '../sections';

export default class CRF extends Component {
    render() {
        const relativeToRoot = this.props.path.match(/\//g).slice(1).map(_ => "../").join("");
        const toThisChapter = relativeToRoot + chapters.tagging.route.slice(1);
                
        return <div>
            <p>
                Directed Graphical Models (or Bayesian Networks)
                are statistical models that model some probability distribution
                over variables <F {...this.props} l="v"/> in a set <F {...this.props} l="V"/> which take values from a set <F {...this.props} l="\mathcal{V}"/>.
                Loosely speaking, Directed Graphical Models can be represented as a directed graph <F l="G"/> where
                nodes represent the 
                variables <span 
                className="avoid-line-break"><F 
                {...this.props} l="v \in V"/>,</span> and 
                the edges represent dependencies.
                Directed graphical models
                factorize as follows:
                <F {...this.props} l="p(V)=\prod _{v\in V}p(v|\pi(v))" display="true"/>
                where <F {...this.props} l="\pi(v)"/> are the parents of node <F {...this.props} l="v"/> in graph <F {...this.props} l="G"/>.
            </p>
            <p>
                The class of Hidden Markov Models ({abbrs.hmms}) is one instance of directed models.
                HMMs have a linear sequences of
                observations <F {...this.props} l="\mathbf x=\{x_t\}_{t=1}^T"/> and a linear sequence of
                labels <F {...this.props} l="\mathbf y=\{y_t\}_{t=1}^T"/> (in {abbrs.hmm} parlance, 'hidden
                states'), which are assignments
                of random vectors <F {...this.props} l="X"/> and <F {...this.props} l="Y"/> respectively, 
                and <F {...this.props} l="V = X\cup Y"/>.
                In HMMs, the observations <F {...this.props} l="\mathbf x=\{x_t\}_{t=1}^T"/> are assumed
                to be generated by the labels.
                One example of an application would be speech recognition, in which
                samples of the sound waves can be seen as observations and the actual phonemes as the labels.
            </p>

            <p>
                To assure computational tractability, {abbrs.hmms} make use of the Markov assumption,
                which is that:
            </p>

            <ol>
                <li>any label <F {...this.props} l="y_t"/> only depends
                    on <F {...this.props} l="y_{t-1}"/>, where the initial probability <F {...this.props} l="p(y_{1})"/> is given
                </li>
                <li>any observation <F {...this.props} l="x_t"/> only depends on the label <F
                    l="y_t"/>;
                    the observation <F {...this.props} l="x_t"/> is generated by label <F {...this.props} l="y_t"/>.
                </li>
            </ol>

            <p>A {abbrs.hmm} then factorizes as follows:

                <F {...this.props} display="true"
                   l="p\left (\mathbf x,\mathbf y \right )= \prod _{t=1}^T p(x_t)p(y_t) = \prod _{t=1}^T p(x_t|y_t)p(y_t|y_{t-1})"/>
            </p>

            <p>
                If we return to the representation of {abbrs.hmms} in <FigRef fig={figs.graphicalModels}/>, we see that
                the
                white nodes represent labels
                and the grey nodes represent
                the observations. Typically, observations are given and the labels need to be inferred. This is
                done from a given HMM by
                looping over all assignment vectors <F {...this.props} l="\mathbf y\in Y"/> and 
                selecting <F {...this.props} l="\mathbf y^*\in Y"/> with the
                highest likelihood.
            </p>

            <p>
                To find a model with plausible values of <F {...this.props} l="p(x_t|y_t)"/> and <F
                l="p(y_t|y_{t-1})"/>, we
                typically perform a parameter estimation method such as the <a
                className="wiki"
                href="https://en.wikipedia.org/wiki/Baum%E2%80%93Welch_algorithm">Baum-Welch
                algorithm</a> on a set of pre-tagged
                observation-label sequences ({ref.cite(bib.lucke1996stochastic)}). This is called training
                the model.
            </p>
            
            <p>
            The procedures for inference and parameter 
            estimation for {abbrs.hmms} are very 
            similar to those for {abbrs.lccrfs} and are explain in more
            depth <a href={toThisChapter+"#"+sectionz.linearChain.id}>in the section on {abbrs.lccrfs}</a>.
            </p>
        </div>;
    }
}
            